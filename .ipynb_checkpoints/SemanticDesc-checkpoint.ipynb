{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd45073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import utils.io, utils.hpatches, utils.features\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caba41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import kornia as K\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "class NaiveSemantic():\n",
    "    def __init__(self, model='deeplabv3_mobilenet_v3_large', contrastThreshold=0.04, edgeThreshold=10):\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda:0')\n",
    "\n",
    "        self.localExtractor = cv2.SIFT_create(nfeatures=0, contrastThreshold=contrastThreshold, edgeThreshold=edgeThreshold, nOctaveLayers=3)\n",
    "        self.semanticExtractor = torch.hub.load('pytorch/vision:v0.10.0', model, pretrained=True).eval().to(self.device)\n",
    "        self.semanticExtractor.classifier = nn.Sequential(*list(self.semanticExtractor.classifier.children())[:-1])\n",
    "        self.semanticExtractor.aux_classifier = nn.Sequential(*list(self.semanticExtractor.aux_classifier.children())[:-2])\n",
    "\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: K.color.bgr_to_rgb(x)),\n",
    "            transforms.Lambda(lambda x: x.to(self.device)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            transforms.Resize(799, max_size=800)\n",
    "        ])\n",
    "\n",
    "    def semanticMap(self, img):\n",
    "        input_tensor = self.preprocess(img)\n",
    "        input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "            output = self.semanticExtractor(input_batch)['out'][0]\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def extractSemantic(self, img, np_kps):\n",
    "\n",
    "        output = self.semanticMap(img)\n",
    "        output = F.interpolate(output.unsqueeze(0), size=(img.shape[0], img.shape[1]), mode='bilinear', align_corners=True).squeeze(0)\n",
    "\n",
    "        semantic_features = output[:, np_kps[:,1], np_kps[:, 0]].T.cpu().numpy()\n",
    "        semantic_features = semantic_features / np.max(semantic_features, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        return semantic_features\n",
    "\n",
    "    def detectAndCompute(self, img, opt=None):\n",
    "        if len(img.shape) == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = img\n",
    "            \n",
    "        kps, local_features = self.localExtractor.detectAndCompute(gray, opt)\n",
    "        if len(kps) == 0:\n",
    "            return [], []\n",
    "        \n",
    "        np_kps = np.array([[kp.pt[0], kp.pt[1]] for kp in kps]).astype(int)\n",
    "        local_features = local_features/local_features.max()\n",
    "\n",
    "        semantic_features = self.extractSemantic(img, np_kps)\n",
    "        features = np.hstack([local_features, semantic_features])\n",
    "\n",
    "        return kps, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d1b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustDesc(desc, proportion):\n",
    "    if len(desc) == 0:\n",
    "        return []\n",
    "\n",
    "    visual = desc[:, :128]\n",
    "    semantic = desc[:, 128:]\n",
    "    \n",
    "    new_total = 128\n",
    "    semantic_size = int(new_total*proportion)\n",
    "    visual_size = new_total - semantic_size\n",
    "\n",
    "    if visual_size > 0:\n",
    "        new_visual = cv2.resize(visual,     (visual_size, visual.shape[0]))\n",
    "    \n",
    "    if semantic_size > 0:\n",
    "        new_semantic = cv2.resize(semantic, (semantic_size, semantic.shape[0]))\n",
    "\n",
    "    if visual_size == 0:\n",
    "        return new_semantic\n",
    "    \n",
    "    if semantic_size == 0:\n",
    "        return new_visual\n",
    "\n",
    "    new_desc = np.concatenate([new_visual, new_semantic], axis=1)\n",
    "\n",
    "    return new_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2868db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher()\n",
    "\n",
    "def makeMatch(kps1, desc1, kps2, desc2, idx=False, dmatch=False):\n",
    "\n",
    "    if len(desc1) == 0 or len(desc2) == 0:\n",
    "        if dmatch:\n",
    "            return dmatches\n",
    "        return [], []\n",
    "\n",
    "    matches = bf.knnMatch(desc1,desc2,k=2)\n",
    "\n",
    "    mkpts1 = []\n",
    "    mkpts2 = []\n",
    "    # Apply ratio test\n",
    "    idx1 = []\n",
    "    idx2 = []\n",
    "    dmatches = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.9*n.distance:\n",
    "            dmatches.append(m)\n",
    "            mkpts1.append(kps1[m.queryIdx])\n",
    "            mkpts2.append(kps2[m.trainIdx])\n",
    "            idx1.append(m.queryIdx)\n",
    "            idx2.append(m.trainIdx)\n",
    "    if dmatch:\n",
    "        return dmatches\n",
    "\n",
    "    if idx:\n",
    "        return np.array(idx1), np.array(idx2)\n",
    "\n",
    "    return np.array(mkpts1), np.array(mkpts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45442f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMatch(mkpts2, gtkpts, total, K=10):\n",
    "    if len(mkpts2) == 0:\n",
    "        return np.zeros(K), np.zeros(K)\n",
    "\n",
    "    diff = mkpts2.astype(int) - gtkpts.astype(int)\n",
    "    diff = np.linalg.norm(diff, axis=1)\n",
    "    \n",
    "    ms = np.zeros(K)\n",
    "    mma = np.zeros(K)\n",
    "    for i, k in enumerate(range(K)):\n",
    "        acc_at_k = (np.abs(diff) <= k).sum()\n",
    "        ms[i] = acc_at_k/total\n",
    "        mma[i] = acc_at_k/len(mkpts2)\n",
    "\n",
    "    return ms, mma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60892479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
